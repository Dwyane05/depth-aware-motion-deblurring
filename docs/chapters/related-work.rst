The works on motion deblurring can be divided into single image deblurring and multi-image deblurring. Furthermore they can be distinguished into the assumption on blur kernels: spatially uniform and non-uniform blur kernels.

In single image deblurring Fergus *et al.* :cite:`Fergus2006` proposed a variational Bayesian approach to estimate the blur kernel by maximizing the marginal probability. Levin *et al.* :cite:`Levin2011` suggested an improved efficient marginal likelihood approximation. Several other approaches :cite:`Joshi2008`, :cite:`Cho2009`, :cite:`Xu2010` altered the MAP problem for estimating the latent image and the blur kernel iteratively by predicting sharp edges using filters. Or reweighting strong edges like Shan *et al.* :cite:`Shan2008`. In non-blind deconvolution where the PSF is known, sparse natural image priors were used to improve image restoration. So Iteratively Re-weighted Least Squares (IRLS) :cite:`Levin2007` and variable substitution schemes :cite:`Shan2008` were employed to constrain the solution. A method to improve estimated PSFs is shown by Zhu *et al.* :cite:`Zhu2012` where the PSF is also deconvolved because itself is affected by blur.

These methods assumes that all the pixels are blurred with the same (uniform) blur kernel. Though blur at a specific position is highly correlated with camera motion and the corresponding scene depth. This results in a spatially non-uniform kernel. Joshi *et al.* :cite:`Joshi2010` discussed the spatial variance of PSFs which is correlated to rotational and translational camera motion. Whereas the spatial variance caused by rotation is depth independent and that caused by translation depends on scene depth.

Some methods deals with rotation blur models such as Shan *et al.* :cite:`Shan2007` and Whyte *et al.* :cite:`Whyte2010`. A simplified camera motion model with 3-DoFs were also introduced by Gupta *et al.* :cite:`Gupta2010`. However camera translation which causes significant image blur isn't handled in these approaches. Because translation depends on scene depth some works already tried to compute depth maps by multi-image approaches. Favaro *et al.* :cite:`Favaro2004` modeled motion-blur and defocus of a scene from a collection of blurred images as an anisotropic diffusion process. Sorel *et al.* :cite:`Sorel2008` proposed an algorithm that belongs to the group of variational methods that estimate simultaneously a sharp image and a depth map, based on the minimization of a cost functional. It uses multiple blurred images and a user selection of a region of constant depth as input.
Ji *et al.* :cite:`Ji2012` on the other hand introduced a region based method to remove spatially-varying blur from a single photograph. Another algorithm from Hu :cite:`Hu2014` works on a single image and provides a unified layer-based model for depth-involved deblurring.

There are also works that take both rotation and translation into consideration like Joshi *et al.* :cite:`Joshi2010` where a inertial sensor is used to measure the camera motion. Tai *et al.* :cite:`Tai2010` used labeled cues to estimate 6-DoF camera motion. Bae *et al.* :cite:`Bae2013` combined a depth sensor (Microsoft Kinect) and an inertial sensor to recover the real camera movement and estimate the spatially-variant blur kernel and the latent image. These approaches have the limitations that they either need external input like user interactions or hardware or they simplify the camera parameter.

There are also works focused on removal of blur caused by object motion :cite:`Jia2007`, :cite:`Chak2010`, :cite:`Kobayashi2014`. However we deal only with motion blur caused by camera motion combined with depth scenes.